# Week 4: Materials (Physically Based and Non-Photorealistic Rendering)
 
Materials control how a 3D Mesh looks when it's rendered on to your screen. A simple mesh's appearance can be vastly altered by attaching different materials on them. For example, check out how this basic model can appear using four different materials:
 
<img src="https://i.imgur.com/9PyHpzv.png">
 
Whether you are an artist or programmer primarily, materials are an important topic to understand. I'll be going over the philosophy around materials in the industry today, and attach some links in case you want to do any further research :)
 
Materials are a cornerstone for all 3D applications, whether it be a video game, animated film, or digital design. It's a rapidly advancing field, but I'm going to very broadly lump them into two main categories you'll see within the industry, **Physically Based Rendering (PBR)** and **Non-Photorealistic Rendering (NPR)**
 
# Physically Based Rendering (PBR)
 
A Physically Based material is programmed with the intention to replicate (as closely as possible) how light actually interacts with different types of matter. There are certain physical "rules" you have to follow in order to write a physical based material. For example, all physically based materials should absorb and reflect the same amount of light energy as it receives (IE, it needs to be energy conserving). These materials also expose intuitive parameters so you can tweak the look to your desire. Whether you are rendering wood, plastic, metal, or anything else, the idea behind PBR is that if you understand the physical properties behind the object you are creating, you'll be able to tweak the parameters to get a realistic depiction of it in your render.	
 
Physically based materials are the industry's default material for the most part. Open up any 3D Application or Game Engine and you'll almost certainly see a physically based inspired material used as a default whenever you create an object. However, while many applications use very similar parameters, there is no "universal" PBR material. Every application generally makes slight modifications as they see fit. This is also why you might find it to be tricky to transfer material information between ZBrush, Blender, 3DSMax, Unity, etc. They are all using their own implementation of a PBR inspired material, possibly with different parameters. That being said, they _do_ tend to share a lot of common parameters which I'll summarize:
 
### Albedo (or Diffuse)
The overall base color across the surface. Usually this set by a texture through the process of [UV Mapping](https://en.wikipedia.org/wiki/UV_mapping)
 
<img src="https://i.imgur.com/tC05V2A.png">
 
### Roughness (or Smoothness)
How rough the object is at a microscopic level (much smaller than what we can render in the size of a pixel). The smoother the material is, the more it reflects light in only one direction, acting similar to a mirror. The rougher it is, the more it will reflect light from all incoming directions.
 
<img src="https://i.imgur.com/vAlDrM2.png">
 
### Metallic
How "metallic" the surface is. The more metallic a surface is the less it refracts light, and instead that light energy is absorbed. The opposite of metallic objects are called dielectrics which are materials like wood or skin. The ability to set a value of "metallicity" is absolutely an approximation since in real life materials are generally either a metal or they aren't, there isn't any in-between. But for the sake of flexibility in your renders this is usually implemented as a slider so you can choose the percentage of "metallicity" in your material.
 
<img src="https://i.imgur.com/6ps8jZQ.png">
 
### Emission
How much light is emitting from inside the material itself. This is usually paired with other effects in order to convey the emissive material. In the example below, the glare around the sphere is actually a post processing effect that scales with the intensity of the emission value. Often the emission is used when baking [lightmaps](https://en.wikipedia.org/wiki/Lightmap) or [light probes](https://docs.unity3d.com/Manual/LightProbes.html) so the light bounces on to other materials in your scene.
 
<img src="https://i.imgur.com/jEOjszh.png">
 
### Putting it all together
Most of these parameters are usually set by textures (created by 3D artists) so they can define different areas in a material to have different values of roughness or metallicity. There is _a lot_ of software to do this, but in my personal opinion [Substance Painter](https://www.substance3d.com/products/substance-painter/) and [Substance Designer](https://www.substance3d.com/products/substance-designer/) tend to be the most common way to create different material parameter textures 

<img src="https://i.imgur.com/B27MvK3.png">
 
(3D model credited to [toomanydemons](https://sketchfab.com/toomanydemons))

### Physically Plausible Materials
Before PBR took over the industry, it was common to see a physically _plausible_ lighting equation called [Phong or Blinn-Phong](https://en.wikipedia.org/wiki/Blinn%E2%80%93Phong_reflection_model). Rather than worrying about energy conservation and realistic reflection models, it aims to approximate how objects in real life with a simple lighting model which is much cheaper to compute (you might still see it around for that very reason!). While these materials aren't technically "physically based", they are useful to know, and their techniques can be applied to many non-photorealistic rendering applications as well!

<p align="center">
 <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/6/6b/Phong_components_version_4.png/655px-Phong_components_version_4.png">
</p>

# Non-Photorealistic Rendering (NPR)
 
Alternatively, you'll often find that you don't want things to be physically based. You'll want the flexibility and creativity to stylize our renders in cartoonish or imaginative ways. This is considered non-photorealistic rendering and you'll hear the acronym NPR a lot when talking about these techniques. Check out some of the effects used in NPR workflows:
 
The Legend of Zelda: Breath of the Wild, uses a common toon lighting effect called [Cel Shading](https://en.wikipedia.org/wiki/Cel_shading).
<p align="center">
  <img src="https://images.igdb.com/igdb/image/upload/t_original/mepn5qujlccdlkhfkwr8.png" width="800" height="450">
</p>
 
[Return of the Obra Dinn](https://store.steampowered.com/app/653530/Return_of_the_Obra_Dinn/) uses a very cool [dithering](https://en.wikipedia.org/wiki/Dither) filter over their meshes.
<p align="center">
  <img src="https://images.igdb.com/igdb/image/upload/t_original/hv9akfgjm5zrmxsb8r8q.png" width="800" height="450">
</p>
 
[Journey's](https://thatgamecompany.com/journey/) sand looks very pretty, although according to their [tech write up]("https://www.alanzucconi.com/2019/10/08/journey-sand-shader-1/") on it, very little of it was actually based on how sand looks in real life. Instead they treated it similar to a liquid / ocean so the character could feel like they are surfing along the sand dunes.
<p align="center">
  <img src="https://www.alanzucconi.com/wp-content/uploads/2019/07/journey_screenshot_05.png" width="800" height="450">
</p>
 
There are a lot less rules to non-photorealistic rendering. While these types of materials almost always still have parameters for artists to tweak, they are usually written specifically for the project or environment by technical artists or graphics programmers. This is done by writing your own material renderer program called [shaders](https://en.wikipedia.org/wiki/Shader)
 
### Shaders
Shaders are such a huge and broad topic that they could probably be discussed in their own weekly material, but I'll give a very brief overview of what they actually are.
 
Shaders are simply a program that runs on your GPU and manipulates a large array of identical data structures at the same time. While this can be used for a lot of different cases, the main uses cases for rendering an object are **Vertex Shaders** and **Fragment Shaders**.  
 
* **A Vertex Shader** runs a function over every vertex on a 3D Mesh. It receives the vertice's information like position, uv coordinates, or normal. It computes whatever data it needs to (usually just modifies the vertex position if desired) and passes all the data into the fragment shader. You might use a vertex shader in order to animate the vertices of a plane in an ocean wave like pattern.
 
<p align="center">
<img src="https://i.imgur.com/PLlSGln.png" width="70%">
 </p>
 
 
* **A Fragment Shader** runs a function over every pixel the rendered 3D object occupies on the screen, and it returns an RBGA value which determines the color that will be rendered on that particular pixel. By writing your own fragment shader, you can implement your own physically based shader or non-photorealistic shader. It  might require both a creative and mathematical mindset, but the possibilities are endless :) 
 
<p align="center">
<img src="https://www.shadertoy.com/media/shaders/MdX3Rr.jpg">
</p>
 
Note: that image above is rendered using _**only**_ a fragment shader (no mesh data or cpu code)! It is running the same function over every pixel in the frame. [You can check out the animated version + code right here!](https://www.shadertoy.com/view/MdX3Rr)
 
 
# Links for further information
* [Super detailed post on the theory behind physically based materials](https://learnopengl.com/PBR/Theory) 
* [Implementation details on coding a physically based renderer](https://learnopengl.com/PBR/Lighting) 
* [GDC Vault contains tons of free videos on PBR and NPR techniques](https://www.gdcvault.com/search.php#&conference_id=&category=free&firstfocus=&keyword=material)
* [Shadertoy hosts many open source fragment shaders to browse and learn from!](https://www.shadertoy.com/)

